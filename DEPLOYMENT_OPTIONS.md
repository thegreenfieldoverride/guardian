# Liberation Guardian: Deployment Options

## ğŸ¯ **Choose Your Liberation Level**

Liberation Guardian offers multiple deployment strategies. Pick what works for YOUR situation.

## â˜ï¸ **Cloud AI (Recommended for Most)**

**Best for:** Small teams, getting started, minimal resource usage

```yaml
# docker-compose.yml
services:
  liberation-guardian:
    image: liberation/guardian:latest
    environment:
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
```

**Trade-offs:**
- âœ… **Tiny footprint** - 150MB image, 256MB RAM
- âœ… **Fast startup** - Ready in 30 seconds
- âœ… **Latest models** - Always using cutting-edge AI
- âœ… **Zero maintenance** - No model updates needed
- ğŸ’° **Cost**: $0-5/month (free with Gemini tier)
- ğŸ“¡ **Privacy**: Data sent to AI providers (Google/Anthropic)

## ğŸ  **Local AI (Maximum Privacy)**

**Best for:** Compliance requirements, air-gapped environments, ultimate privacy

```yaml
# docker-compose.local.yml
services:
  liberation-guardian:
    image: liberation/guardian:local
    environment:
      - AI_PROVIDER=local
      - LOCAL_MODEL_URL=http://ollama:11434
    depends_on:
      - ollama
      
  ollama:
    image: ollama/ollama:latest
    volumes:
      - models:/root/.ollama
    command: ollama serve qwen2.5:7b
    deploy:
      resources:
        limits:
          memory: 12G
          cpus: '4.0'
```

**Trade-offs:**
- ğŸ”’ **Complete privacy** - Zero external API calls
- ğŸ’° **Zero ongoing cost** - No API fees ever
- ğŸ›¡ï¸ **Air-gapped capable** - Works with no internet
- ğŸ›ï¸ **Full control** - Own your models and data
- ğŸ“¦ **Large footprint** - 6-12GB RAM, 4+ CPU cores
- â±ï¸ **Slower startup** - 5-10 minutes for model loading
- ğŸ”§ **More maintenance** - Model updates, resource tuning

## âš–ï¸ **Hybrid AI (Best of Both)**

**Best for:** Teams that want flexibility and redundancy

```yaml
# docker-compose.hybrid.yml
services:
  liberation-guardian:
    image: liberation/guardian:hybrid
    environment:
      # Primary: Local model
      - AI_PROVIDER=local
      - LOCAL_MODEL_URL=http://ollama:11434
      
      # Fallback: Cloud models
      - FALLBACK_PROVIDER=google
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      
      # When to fallback
      - FALLBACK_ON_HIGH_LOAD=true
      - FALLBACK_ON_MODEL_UNAVAILABLE=true
```

**Trade-offs:**
- ğŸ¯ **Local-first** - Uses private model when possible
- ğŸ”„ **Cloud fallback** - Switches to cloud if local overloaded
- ğŸ’° **Variable cost** - $0-5/month depending on fallback usage
- ğŸ“¦ **Medium footprint** - Local model + small cloud usage

## ğŸ“Š **Comparison Matrix**

| Feature | Cloud AI | Local AI | Hybrid AI |
|---------|----------|----------|-----------|
| **Monthly Cost** | $0-5 | $0 | $0-3 |
| **RAM Usage** | 256MB | 6-12GB | 6-12GB |
| **CPU Usage** | Low | High | High |
| **Startup Time** | 30s | 5-10min | 5-10min |
| **Privacy** | Medium | Maximum | High |
| **Compliance** | Depends | âœ… Full | âœ… Full |
| **Maintenance** | None | Medium | Medium |
| **Internet Required** | Yes | No | Optional |

## ğŸ¯ **Recommendation Guide**

### **Choose Cloud AI If:**
- ğŸƒ **Getting started** - Want to try Liberation Guardian quickly
- ğŸ’» **Resource constrained** - Limited RAM/CPU available
- ğŸŒ **Internet always available** - Reliable external connectivity
- ğŸ‘¥ **Small team** - Under 10 developers
- ğŸ’° **Cost conscious** - Want predictable low monthly cost

### **Choose Local AI If:**
- ğŸ”’ **Compliance requirements** - HIPAA, SOC2, government
- ğŸ›¡ï¸ **Maximum privacy** - Data cannot leave your infrastructure
- ğŸ¢ **Enterprise security** - Air-gapped or restricted networks
- ğŸ’° **High volume** - Processing 1000s of events daily
- ğŸ›ï¸ **Complete control** - Want to own entire AI stack

### **Choose Hybrid AI If:**
- âš–ï¸ **Best of both** - Want local privacy with cloud reliability
- ğŸ“ˆ **Variable load** - Sometimes high volume, sometimes low
- ğŸ”„ **Redundancy** - Want backup if local model fails
- ğŸ§ª **Experimenting** - Testing local models before full commitment

## ğŸš€ **Quick Start Commands**

### **Cloud AI (5 seconds)**
```bash
curl -sSL https://get.liberation.dev | bash -s -- --cloud
```

### **Local AI (5 minutes)**
```bash
curl -sSL https://get.liberation.dev | bash -s -- --local --model qwen2.5:7b
```

### **Hybrid AI (5 minutes)**
```bash
curl -sSL https://get.liberation.dev | bash -s -- --hybrid
```

## ğŸ’¡ **Resource Planning**

### **Cloud AI Requirements**
- **RAM**: 512MB minimum, 1GB recommended
- **CPU**: 1 core minimum, 2 cores recommended
- **Storage**: 1GB for logs and data
- **Network**: Reliable internet connection

### **Local AI Requirements**
- **RAM**: 8GB minimum, 16GB+ recommended
- **CPU**: 4 cores minimum, 8+ cores recommended  
- **Storage**: 20GB for models, 5GB for data
- **Network**: Optional (can run air-gapped)

### **Hybrid AI Requirements**
- Same as Local AI for hardware
- Reliable internet for fallback scenarios

## ğŸ›ï¸ **Liberation Principle**

**You choose what works for YOUR situation.** 

- Need to start fast and cheap? â†’ Cloud AI
- Need maximum privacy? â†’ Local AI  
- Want flexibility? â†’ Hybrid AI
- Want to change later? â†’ Easy migration between all options

**No vendor lock-in. No forced upgrades. No hidden costs. Just working autonomous operations that respect your constraints.** ğŸš€